\documentclass{article}
\usepackage{amsmath,amssymb,bbm, url,color}
\usepackage[hidelinks]{hyperref}

\addtolength{\textwidth}{2cm}
\addtolength{\hoffset}{-1cm}
\addtolength{\textheight}{2cm}
\addtolength{\voffset}{-1cm}

\newcommand{\mc}{\mathcal}
\DeclareMathOperator*{\argmax}{arg \, max}


\begin{document}
\title{
{\normalsize Lab sessions for the class}\\[-1mm]
Distributed Algorithms and Network Systems\\[-2mm]
{\normalsize MiSCIT Master -- Grenoble -- fall 2015}\\[-1mm]
{\normalsize prepared by Federica Garin and Ga\"{e}tan Harter}}
\date{}
\maketitle
\vspace{-2cm}

\textit{NB: This is a first version of this document, only describing in details the first half of the lab activities. The full description will be available later.}


\section{About this lab and the report}
In this lab, you will put in practice some distributed algorithms seen in class.
You will use the experimental platform FIT/IoT-lab (\url{https://www.iot-lab.info}).
What is it? Read the answer here: \url{https://www.iot-lab.info/what-is-iot-lab/}.\\

You are required to write a report, which will be graded.\\ Send it by e-mail to \texttt{federica.garin@inria.fr}.\\
Deadline: Tuesday, Dec.~22nd.\\

Recommendations for a good report:
\begin{itemize}
\item Make sure to follow the instructions given in this document, and include all required items.
\item Write in clear English and precise math. Justify and explain all your statements, but be concise.
\item Figures require a caption. Plots require axes labels and legend.
\item Please include in the report a copy-paste of the relevant lines of C code that you have written.
\end{itemize}


\section{Get familiar with FIT/IoT-lab}


\textit{\color{blue} This preliminary task is not meant to be described in your report. Just write a very short introduction (few lines!), stating the goal of this lab and mentioning the use of FIT/IoT-lab}\\


To familiarize yourself with the use of the platform, go to the page of tutorials:
\begin{center} \url{https://www.iot-lab.info/tutorials/} \end{center}
and do the following tutorials (First steps):
\begin{itemize}
\item Configure your SSH access;
\item Submit an experiment with Web portal: M3 nodes;
\item Get and compile a firmware code for a M3 node.
\item Tools$\to$Node-cli client (Update firmware from command line)
\end{itemize}

\subsection{Provided files}

%For this lab, you are given
%an already-written firmware that allows you to interact with the nodes, and which has two modes, either synchronous or gossip.
%In synchronous mode, there is a global notion of discrete-time iterations: at each iteration, all communications take place, then all local computations are done.
%In gossip mode, at each iteration a random node is selected to send its state; all neighbors having received such message immediately proceed to do a local computation and update local state.
For this lab, you are given
an already-written firmware that allows you to interact with the nodes, either in synchronous or gossip mode. You will start by using the synchronous mode, where  there is a global notion of discrete-time iterations: at each iteration, all communications take place, then all local computations are done.

%In gossip mode (more precisely, this is a broadcast gossip), each node has a local Poisson clock; when a node is activated by its Poisson clock, it sends its state to its neighbors; all neighbors having received such message immediately proceed to do a local computation and update local state.

%TBD FEDE add appendix about Poisson clock

%TBD do we keep two different gossip, or only the poisson-clocks one???

There are python scripts already written for you; the thing you will need to do
is to open some C file and modify it, so as to write the suitable functions for
the required computations.

\subsection{Accesing file on the server}

You should:
\begin{itemize}
\item Follow the instructions in the tutorial to configure your ssh access
\item Open a  terminal, do \verb=ssh=, to arrive at \verb=<username>@grenoble:~= and then change directory:\\
\verb=cd ~/iot-lab/parts/openlab/appli/iotlab_tests/distributed_computing= \\
This is where the files are sitting, that can be modified by you.\\

Use \verb=ls= to see which files are there. The ones you need are the following:
	\begin{itemize}
	\item Mainly, you will need to modify \verb=computing.c= to add the functions for computing (more details below)
	\item In the file \verb=config.h=, you will need to modify:
		\begin{itemize}
%        \item The number of values (i.e., how many scalars there are in the state
%            of each node). This is the constant \verb=NUM_VALUES= defined
%            quite at the beginning of the file. You will need $1$ when doing
%            scalar consensus, you will need more when doing max-consensus
%            for node-counting. Choose an integer between $1$ and $14$.

		\item The channel: to avoid interfering with other groups, set the
            constant \verb=CHANNEL= to different numbers, group $x$ will take
            channel $10+x$, e.g., group 2 gets channel $12$
		\item Possibly, you might  need to modify another thing in this file:
            the power reception threshold \verb=MIN_RSSI= in case the graph is
            too sparse (disconnected) or too dense (too many communications, not
            an interesting example of distributed computing). This signal
            strength is in dB, and there is a minus sign before the value,
            so putting a higher value means decreasing strength.
		\end{itemize}
	\end{itemize}
You can open and modify the files directly on the server, without downloading
them to your local computer, just use \verb=nano <filename>.c= to open a file in
a text editor (to save the file after having modified it, follow instructions to
quit and then you will be prompted with the question where you wish to save it)

\item About the functions in \verb=computing.c=
	\begin{itemize}
	\item  \verb=init_value()= does random initialization; this is for a scalar value.
%        It is called multiple times if \verb=NUM_VALUES=$\ge 2$ (remember,
%        this parameter is set in the file \verb=config.h=)
	\item \verb=compute_value_from_neighbours= has to be used only in the synchronous mode.
        It allows a node to compute its new state given: its own old state (\verb=my_value=),
        its own degree (\verb=my_degree=), and a structure received from neighbors,
        containing their state and degree. Notice: if you are able to add the
        contribution of each neighbor incrementally, you can use the
        already-written code to parse the structure of received values, so that
        you do not need to worry about that: at iteration $i$, which concerns
        the neighbor $i$, \verb=neighbour_degree= is the degree of $i$ and
        \verb=neighbour_value= is the value (the state) of $i$.
        % Also notice: when there is more than one state per node (\verb=NUM_VALUES=$\ge 2$), this function is called many times, one per each state (it operates on a scalar state, not vector).
	\item \textit{Other functions will be used in the gossip part of the lab and will be described in version 2 of this document}
%	\item \verb=compute_value_from_gossip= has to be used only in gossip mode. It allows a node to compute its new state, given its old state and the received state (received from a neighbor having done the gossip broadcast transmission).
%	\item \verb=compute_final_value= is needed when computing the estimated number of nodes, after the gossip max-consensus algorithm
	\end{itemize}

\end{itemize}

\subsection{Running experiments for the exercices}

\paragraph{First experiment} Book nodes for your experiment as in the tutorial.
\begin{itemize}
\item Duration: 180~minutes
\item Select nodes by Type
\item Archi: \texttt{m3:at86rf231}
\item Site: Grenoble
\item Number: $30$
\end{itemize}
but do not associate (yet) any firmware with the nodes.

\paragraph{Next weeks experiments} On the web portal, click on your last
experiment and click on \textbf{Reload}. This will ensure you are using the
same nodes. If you use different nodes you will have to re-generate the network
graph on new experiments (explained later).

\subsubsection{Commands}

\begin{itemize}

\item Compile: every time you want to compile, from the directory\\
    \verb=<username>@grenoble:~/iot-lab/parts/openlab/build.m3$=\\
    simply type \verb=make distributed_computing=

    \begin{verbatim}cd ~/iot-lab/parts/openlab/build.m3
make distributed_computing\end{verbatim}

\item After having compiled, you need to flash your executable file to all nodes.
    To do so, from the same directory run:
    \begin{verbatim}cd ~/iot-lab/parts/openlab/build.m3
node-cli --update bin/distributed_computing.elf\end{verbatim}

\item Generating communication graph. This creates the communication graph that
    will be used for your experiments. It can be generated once and for all and
    reused across experiments.

    \begin{verbatim}cd ~/iot-lab/parts/openlab/appli/iotlab_tests/distributed_computing/
# Generate graph
./network_graph_create.py  -o ~/communication_graph

ls ~/communication_graph
graph.dot  graph.png  neighbours.csv \end{verbatim}

It generates two important files:
\texttt{neighbours.csv} which will be used by the algorithms and
\texttt{graph.png} which is the graphical representation for visualization.

Download graph and visualize it:
\begin{verbatim}scp -r <username>@grenoble.iot-lab.info:~/communication_graph/ ./
eog ~/communication_graph/graph.png\end{verbatim}

If you graph is not connected or too dense you may want to generate another one.
First change the \verb=config.h -> MIN_RSSI= value. Decreasing the value
increases connectivity. Then compile, flash and re-run the graph generation.


\end{itemize}


\subsection{Running experiments for the exercices}

Run your experiment: in a terminal where you are at \\
\verb=<username>@grenoble:~/iot-lab/parts/openlab/appli/iotlab_tests/distributed_computing/= \\
Type:
\begin{itemize}
	\item \verb=./algo_XXXX.py --num-loop 50 -g ~/communication_graph/neighbours.csv --outdir ~/ex_1/= \\
    Use the correct script according to the current algorithm \verb=algo_syncronous.py=, \verb=algo_gossip.py=~\dots
	The option \verb=--num-loop= gives the number of iterations, the option \verb=--outdir= describes the folder for the results files.\\
	The option \verb=-g= gives the communication graph to use for the algorithm.\\
	\end{itemize}

Download the results: open another terminal, on which you are not going to do ssh, just stay on your home. Type \\
         \verb=scp -r <username>@grenoble.iot-lab.info:~/ex_1/ ./=\\
where \verb=ex_1=  needs to be replaced by the suitable name you had chosen for your results folder while launching the experiment.

\paragraph{}The results are the following. \\
Then there is, for each node, a .csv file, with a first column indicating the
iteration number, and next columns containing the value(s) of the state(s) at
such time;
%the number of such columns is determined by \verb=NUM_VALUES= (in file \verb=config.h=),
while the number of iterations is chosen with the option \verb=--num-loop= when
running the experiment.
You can either use these individual files (one per node), or use the file
\verb=results_all.csv= where results from all nodes are collected:
this one has a first column with the node label, then a column with the iteration number,
and then next columns containing the value(s) of the state(s) of such node at such iteration.


\section{Synchronous consensus}
You are given a setup where communications are synchronized (at each iteration,
all communications take place, then all local computations are done) and follow a static communication graph.
You will implement some consensus algorithm (as described in class and in the tutorial \cite{consensus-tutorial}).


\subsection{Synchronous Metropolis LTI average consensus}
\textbf{Algorithm to be implemented:}
Initialize $x(0)$ with some values (the goal is to compute the average of such initial values). Run linear consensus algorithm $x(k+1) = P x(t)$ where the matrix $P$ is a doubly-stochastic matrix consistent with the given communication graph; to construct a $P$ satisfying these requirements, use the Metropolis-Hastings weights. \\
Suggestion: use $30$ nodes and $100$ iterations
%(or better, first run once with very few iterations, just to check the graph construction works correctly, then run again with $100$ iterations)\\

As a reminder, the Metropolis-Hastings weights are defined as follows:
denoting with $d_i$ the number of neighbors of $i$ and with $\mc N_i$ the set of such neighbors,
\[ P_{ij} =
\begin{cases}
\frac{1}{1+\max\{d_i, d_j \}} & \text{if $j \in \mc N_i$,}\\
1-\sum_{k \in \mc N_i} P_{ik} & \text{if $j = i$,}\\
0							  & \text{otherwise}.
\end{cases}
\]
Notice that the state-update $x_i(k+1) = \sum_j P_{ij} x_j(k)$ with the above-defined weights can be equivalently done as follows:
\[ x_i(k+1) = x_i + \sum_{j \in \mc N_i} \frac{1}{1+\max\{d_i, d_j \}} (x_j - x_i) \,.\]

\textit{\color{blue}
Performance analysis (to be included in the report):
\begin{itemize}
\item Plot the trajectories of the states of all nodes. Do they converge? If so, do they converge to a common value, and is this value in accordance with theoretical prediction (hint: use the initial values in $x(0)$)?
\item Look at the graph, and check if it is undirected (all edges bi-directional) and connected. Please include the graph image in the report.
%Also check if the graph seems to be static, by looking e.g. at the degrees of some nodes (degrees are transmitted at each time, and they are supposed to be time-invariant).
\item Given the properties of the graph and the definition of Metropolis weights, what are the properties of matrix $P$? (is it stochastic? symmetric? doubly-stochastic? irreducible? aperiodic? primitive? -- justify your answers)
Explain the above-observed convergence to average consensus (of lack thereof) with these properties of $P$, using results about convergence of powers of stochastic matrices.
\end{itemize}
} %end blue italic

\subsection{More synchronized LTI consensus algorithms, if time permits}
If there is some time left, play around with linear synchronous consensus, by using different matrices, and do the same results analysis as above, so as to compare the final values and the rapidity of convergence. Examples of matrix:
\begin{itemize}
\item Simple random walk: $P = (I+D)^{-1} (I+A)$, where $A$ is the adjacency matrix (of the graph without self-loops), and $D$ the diagonal matrix of degrees.
\item Discretized Laplacian:  $P = I - \alpha L$, for some $\alpha \in (0,\frac{1}{d_{\max}})$, $d_{\max}$ being the maximum of the nodes' degrees (so that diagonal entries of $P$ are ensured to be positive), and where $L=D-A$ is the Laplacian of the graph.
\end{itemize}
Hint: write down the entry-wise version of the equation $x(k+1) = P x(k)$ with the above-defined choices of $P$.

\textit{\color{blue} If you have time to get these results, then add them to the report.}

%TBD Federica - construct and appendix about Poisson clock
%\section{Broadcast gossip}
%You are given a setup where at each iteration one sensor (picked at random, according to independent uniform r.v.s) broadcasts its value to all its neighbors; the neighbors update their state using the received value, while all other sensors (including the one having transmitted) keep their previous state. The graph describing the neighbors is static.\\
%
%Comments:
%\begin{itemize}
%\item Differently from the synchronous case, here only one sensor communicates at each iteration. Hence it is natural that convergence takes much longer number of iterations. What would be a re-scaling of the time axis to have a fair comparison?
%\item This broadcast-gossip setup, where at each iteration one sensor at random transmits (the sensor being selected with uniform probability on the set of all sensors, independently from past selections), is simulated in a way that involves a central scheduler. However, this same probabilistic setup can be obtained in a fully local way, without any centralized scheduler nor time-synchronization algorithm, by the use of independent local randomized clocks. Give a short description of the `Poisson clock' gossip setup, after reading the paragraph `Asynchronous time model' in Sect.~I-A of the paper \cite{gossip-poisson}.
%\end{itemize}


\section{Broadcast-gossip average consensus}

\textit{This task will be described in version~2 of this document}

%TBD fix notation! write about equivalence with or without poisson clock
%
%When a sensor $i$ broadcasts its value, each of its neighbors updates its state, say neighbor $j$ computes an updated state $x_j(k+1)$ using the received $x_i(k)$ and the local value $x_j(k)$. Consider the following  update rule:
% for some fixed $\alpha \in (0,1)$, $x_j(k+1) = (1- \alpha) x_j(k) + \alpha x_i(k) $, or, equivalently,
%$x_j(k+1) = x_j(k) + \alpha (x_i(k)-x_j(k)) $.\\
%
%First, consider the averaging rule (fix a same value $\alpha$ for all nodes, although this is not a crucial point).
%
%\textbf{Algorithm to be implemented:}\\
%Initialize $x(0)$ with some values (the goal is to compute the average of such initial values).
%Run broadcast gossip with the above-described averaging update.\\
%Suggestion: use $30$ nodes and $5000$ iterations, and $\alpha = \frac{1}{2}$.
%
%\textbf{Results analysis:}
%\begin{itemize}
%\item Plot the trajectories $x_i(k)$, for all nodes $i$. Do they converge? if so, do they converge to a common value, and what is this common value?
%\item Look at the graph, and check if it is undirected and connected; also check if it seems to be static (e.g. if the degrees of some nodes remain constant)
%\item Compare the obtained results with theory.
%\end{itemize}
%
%Comments about the relevant theoretical results:
%\begin{itemize}
%\item
%The state update is $x(k+1) = P(k) x(k)$, with $P(k)$ a realization of a random variable. $P(k)$ are i.i.d., uniformly extracted in a finite set of matrices $P^{(1)}, \dots, P^{(n)}$,  where a matrix $P^{(i)}$ corresponds to a node $i$ being extracted and the averaging update being done by $i$'s neighbors. Write explicitly the entries of $P^{(i)}$ for a given $i$ (already done in class).
%\item
%For given initial values $x(0)$, the trajectories of $x(k)$ will be realizations of a random variable, depending on the random sequence in which nodes are activated to communicate. The expected value $\bar x(k) := \mathbb E x(k)$ will have the dynamics $\bar x(k+1) = \bar P \bar x(k)$, where $\bar P := \mathbb E P(k)$ (the expected value is the same for all $k$, since $P(k)$' s are identically distributed).
%Compute $\bar P$ (hint: $\bar P = \sum_{i=1}^n \frac{1}{n} P^{(i)}$);
%try to find a neat expression for $\bar P$, involving the Adjacency or the Laplacian matrix of the graph, and of course also the parameter $\alpha$.
%\item Theory about the expected trajectory $\bar x(k)$.
%Notice that $\bar P$ is a weighted adjacency matrix of the graph describing neighbors; if the graph is rooted (contains a rooted spanning tree), theory predicts convergence of all entries $\bar x(k)$ to a consensus value $\bar \pi^T x(0)$, where  $\bar \pi^T$ is the left dominant eigenvector of $\bar P$, namely $\bar \pi^T \bar P = \pi^T$ and $\sum_i \bar \pi_i = 1$. Look at $\bar P$: what can you say about $\bar \pi^T$? What can you say about the consensus value, in the case where the graph is undirected and connected?
%\item Theory about the random trajectory $x(k)$.
%	\begin{itemize}
%	\item The fact that the expected value $\bar x(k)$ converges to consensus does not imply that the random trajectories $x(k)$ will converge. It actually happens that random trajectories do converge with high probability to a consensus (i.e., a common value) (see theorem below), but this value needs not to be the average of initial values.
%	About convergence of random trajectories, the following theorem holds:\\
%	\textit{Theorem~1 (\cite{gossip-converg-as}, Coroll.~3.2})\\
%	Assume that:
%		\begin{itemize}
%		\item For all node $i$, $P_{ii}(k)>0$ almost surely,
%		\item the graph associated with $\bar P$ is strongly connected.
%		\end{itemize}
%	Then, $x(k)$ achieves probabilistic consensus, i.e.:
%		\begin{itemize}
%		\item If the initial condition is already a consensus $x(0) = a \mathbf 1$ for some scalar $a$, then
%		$x(k) = a \mathbf 1$ for all k;
%		\item given any initial condition $x(0)$, there exists a scalar random variable $x_{\infty}$ (depending on the initial condition), such that $x(k)$ converges almost surely to $x_{\infty} \mathbf 1$.
%		\end{itemize}
%	\hfill $\square$ \\
%	Verify that the assumptions of this theorem are satisfied by the broadcast gossip consensus that we are considering.
%	 \item
%	 Theorem~1 guarantees almost sure convergence to a common consensus value. However, in general the consensus value $x_{\infty}$ is a random variable: for given initial values, it can vary depending on the random sequence $P(k)$. For the broadcast gossip averaging algorithm, if the graph is undirected, in expectation we have $\bar P$ which is doubly-stochastic, so that the expectation $\bar x(k)$ will converge to average consensus, but each random realization $x(k)$ needs not to do so. The following theorem guarantees that the error between the random final value $x_{\infty}$ and the correct expected final value $\frac{1}{n}\sum_i x_i(0)$ cannot be too bad, there is an ensured upper bound for it, described below \textit{(read it, for your information, but you will not need to put it in the report)}. \\
%	 Denote by $\bar x(k) :=  \mathbb E x(k)$ the expectation of the state, and denote by $x_{\text{ave}}(k) := \frac{1}{n}\sum_i x_i(k)$ the scalar random value obtained as the average of the states $x_i(k)$ at a given time $k$; notice that $x_{\text{ave}}(0)$ is not random, and is the desired average of the initial values.
%	 Define $V(0) = \frac{1}{n} \sum_i (x_i(0) - x_{\text{ave}}(0))^2 $; this is a measure of dispersion of the initial values (it is natural to expect that the smaller $V(0)$ is, the easier it is to go to average consensus, since initial values are already near to it). Define $\gamma:= \frac{\alpha}{1-\alpha} d_{\max}$,
%	 where $\alpha$ is the parameter used in the averaging broadcast gossip update and $d_{\max}$ is the maximum of the out-degrees of all nodes (out-degree being the number of neighbors to which a node can broadcast). \\
%	\textit{Theorem~2 (\cite{gossip-small-error}, Theorem~1 and Example~2)}\\
%For the averaging broadcast gossip algorithm and with the above definitions, the following bound holds true:
%\[ \mathbb E [ (x_{\text{ave}}(k) - x_{\text{ave}}(0))^2 ] \le
% \frac{\gamma}{n} V(0)  \, .\]
%If the graph associated with $\bar P$ is strongly connected, then Theorem~1 ensures that
%$x(k)$ converges almost surely to $x_{\infty} \mathbf 1$ for some random variable $x_{\infty}$ and $\bar x(k)$ converges to $x_{\text{ave}}(0) \mathbf 1$;  the random variable $x_{\infty}$ satisfies the following bound:
%\[ \mathbb E [ (x_{\infty} - x_{\text{ave}}(0))^2 ] \le
%\frac{\gamma}{n} V(0)\,. \vspace{-2mm}\]
%\hfill $\square$
%
%
%	\end{itemize}
%\end{itemize}


\section{Broadcast-gossip clock-synchronization}

\textit{This task will be described in version~2 of this document}


\begin{thebibliography}{99}

\bibitem{consensus-tutorial}
F. Garin and L. Schenato, A survey on distributed estimation and control applications using linear consensus algorithms, in {\it Networked Control Systems}, A. Bemporad, M. Heemels, and M. Johansson eds, Springer LNCIS, vol.~406, Chapter~3, pp.~75--107, Springer, 2011, Available on-line: \url{http://necs.inrialpes.fr/people/garin/WIDEbook_GarinSchenato.pdf}


\bibitem{gossip-poisson}
S. Boyd, A. Ghosh, B. Prabhakar, and D. Shah, Randomized gossip algorithms, {\it IEEE Trans.\ on Information Theory}, vol.~52, no.~6, 2006, pp.~2508--2530, Available on-line: \url{http://web.mit.edu/devavrat/www/rand-gossip.pdf}


\bibitem{gossip-converg-as}
F. Fagnani and S. Zampieri, Randomized consensus algorithms over large scale networks,
{\it IEEE Journal on Selected Areas in Communications}, vol.~26, no.~4, 2008, pp.~634-â€“649, Available on-line:
\url{http://calvino.polito.it/~fagnani/coordincontrol/Random%20consensus.pdf}

\bibitem{gossip-small-error}
P. Frasca and J. M. Hendrickx, On the mean square error of randomized averaging algorithms, {\it Automatica}, vol.~49, no.~8, 2013, pp.~2496--2501, Available on-line: \url{http://arxiv.org/pdf/1111.4572.pdf}

\bibitem{RandSync-journal}
S. Bolognani, R. Carli, E. Lovisari, and S. Zampieri,
A randomized linear algorithm for clock synchronization in multi-agent systems,
to appear, {\it IEEE Trans.\ on Automatic Control}, vol.~61, no.~7, 2016.
Preprint available on-line:
\url{http://control.ee.ethz.ch/~bsaverio/papers/randsync.pdf}


\end{thebibliography}




\end{document}
